\section{\texttt{numl/NuGraph/scripts}}

Main components to look at: \texttt{train.py} and \texttt{NuGraph2.py}.

\begin{itemize}
    \item Libraries:
        \begin{enumerate}
            \item \texttt{torch}
                \begin{enumerate}
                    \item \texttt{cat}: Used to concatenate tensors along specified dimension
                    \item \texttt{optim}: Used to minimize the loss $\mathcal{L}$ via gradient descent. Contains lots of useful functions.
                    \item \texttt{nn}: Used to construct neural networks. \texttt{nn.Sequential} is used adundantely and contructs sequential layers.

                    \item \texttt{SimpleConv}: message passing operator that performs (non-trainable) propagation.
                    
                    \item \texttt{torchmetrics.functional}: Torchmetrics is a library for metric computations; highly compatible with PyTorch. Functions contains specific class of functions.
                \end{enumerate}
        \end{enumerate}
        
    \item Loss function $\mathcal{L}(\theta)$. Path: \texttt{NuGraph/util}. Both of these are imported into \texttt{decoders.py}.
    \begin{enumerate}
        \item \texttt{RecallLoss.py}
            \begin{enumerate}
                \item This is a function of the parameters $\theta$ of the model. This funtion is minimized by varying $\theta$.
                
                \item Sets $\texttt{weight} = 1 - recall(\hat{x}_i, y_i)$ where $\hat{x}_i$ is the model output and $y_i$ is the respective label
                
                \item Cross-entropy loss! Computes
                \begin{equation}
                    \begin{aligned}
                        CE(p,  \hat{p}) &= -\sum_ip_i\log(\hat{p}_i) \ \ \ p_i \text{ one-hot labels}, \hat{p}_i \text{ model pred,}\\
                        \mathcal{L}(\theta) &= \text{Avg}_i CE(y_i, \sigma(f(x_i, \theta)))
                    \end{aligned}
                \end{equation}
                wehre $\sigma$ is the non-linear activation function, and $f(x_i, \theta)$ is the models guess based on the input $x_i$ and the current parameters $\theta$. 
            \end{enumerate}

        \item \texttt{LogCoshLoss.py}
    \end{enumerate}

    \item \texttt{train.py}: This is the file that is called by the \texttt{.sh} files. It contains two functions;
        \begin{enumerate}
            \item \texttt{configure}: This will likely stay roughly similar in our treatment
                \begin{enumerate}
                    \item Defines an argument \'parser\' which is used to process (some of) the information that we provide in the command line (log name/directory).

                    \item The line \texttt{parser = Data.add\_data\_args(parser)} calls the function \texttt{add\_data\_args} from the module \texttt{Data}. This function interprets the information we give about the data set in the \texttt{.sh} files (i.e. file name/path, batch size, etc.).

                    \item Similar lines to that above are called to initialize the model using the passed parameters (i.e. hidden layers, epochs, learning rate)
                \end{enumerate}

            \item \texttt{train}: 
                \begin{enumerate}
                    \item Begins by configuring the data set via \texttt{nugraph/data/H5DataModule}
                    \item Calls \texttt{NuGraph2.py} (or resumes training)
                    \item Writes log for Slurm
                    \item Calls \texttt{pl.Trainer}. This is the \texttt{PyTorch Lightning} module that trains the network. This module works with all of them methods contained in the \texttt{NuGraph.py} model. See documentation for LightningModule \href{https://lightning.ai/docs/pytorch/stable/common/lightning_module.html}{\color{blue} here}.
                \end{enumerate}
        \end{enumerate}

    \item \texttt{/nugraph}: A directory that contains the models and data tools that are called in \texttt{train.py}.
        \begin{enumerate}
            \item \texttt{/data}: A directory which contains data configuration methdos used in the model.
                \begin{enumerate}
                    \item \texttt{H5DataModule.py}
                        \begin{enumerate}
                            \item \texttt{\_\_init\_\_}: Grabs all relevant pieces of information from the \texttt{.hdf5} file passed to the model
                        \end{enumerate}
                \end{enumerate}
            \item \texttt{/models}: A directory which contains the networks called in \texttt{train.py}. Note that all of these models utilize the PyTorch library.
                \begin{enumerate}
                    \item \texttt{encoder.py}: Contains a single class with two functions
                        \begin{enumerate}
                            \item \texttt{\_\_init\_\_}
                            \begin{itemize}
                                \item Inputs:
                                    \begin{itemize}
                                        \item $n$: number of input features. I.e. amount of information given to the network for each data sample.

                                        \item $m$: number of hidden features. This is the number of classes (the number of types of particles that are being identified).
                                    \end{itemize}

                                \item A  two-layer sequential neural network with one linear layer and one non-linear layer with a $\tanh$ activation. These layers are applied sequentially;
                                    \begin{equation}
                                        z = \tanh\left(Wx + b\right)
                                    \end{equation}
                                where $x\in \R^n$, $W\in \R^{n\times m}$ and $z, b\in \R^m$. In this context, the matrix $W$ is referred to as the 'weight matrix', and the vector $b$ is refered to as the 'bias'. Both $W, b$ are learned parameters that are optimized by the algorithm. Note that $\tanh$ is applied component-wise.


                                \item This function loops through each readout plane $[u, v, y]$ and applies the sequential NN to each.
                            \end{itemize}
                            
                            \item \texttt{forward}
                            \begin{itemize}
                                \item 
                            \end{itemize}

                        \end{enumerate}

                    \item \texttt{decoder.py}: Contains a multiple classes
                        \begin{enumerate}
                            \item \texttt{DecoderBase}
                                \begin{itemize}
                                    \item This initializes all of the other decoders
                                    \item \texttt{loss}: Function
                                        \begin{itemize}
                                            \item returns loss (number) and metrics (some sort of data allocation)
                                            \item Computes the following:
                                            \begin{equation}
                                                \begin{aligned}
                                                    \underbrace{w = W\exp(-T)}_{T \text{ learned}} && \mathcal{L} = wf(x, y) + T
                                                \end{aligned}
                                            \end{equation}
                                            where $f(x, y)$ is an input parameter to the class.

                                        \end{itemize}
                                \end{itemize}

                            \item \texttt{SemanticDecoder}: Uses \texttt{RecallLoss}

                            \item \texttt{FilterDecoder}: Uses 

                            \item \texttt{InstanceDecoder} (this is in the github, not on compute Canada)

                            \item \texttt{EventDecoder}: Uses \texttt{RecallLoss}

                            \item \texttt{VertexDecoder}: Uses \texttt{LogCoshLoss}
                        \end{enumerate}
        

                    \item \texttt{nexus.py}. Contains two classes, each with multiple functions.
                        \begin{enumerate}
                            \item \texttt{NexusDown}
                                \begin{itemize} 
                                    \item \texttt{\_\_init\_\_}
                                        \begin{itemize}
                                            \item Runs two subnetworks: \texttt{edge\_net} and \texttt{node\_net}

                                            \item \texttt{edge\_net}: Sequential NN consisting of a single linear layer, followed by softmax. Note softmax is a function $\sigma$:
                                            \begin{equation}
                                                (\sigma(x))_i = \frac{e^{x_i}}{\sum_{j=1}^n e^{x_j}}
                                            \end{equation}
                                            Softmax converts vector entries to relative probabilities (notice that $\sum_i (\sigma(x))_i = 1$)

                                            \item \texttt{node\_net}: Applies four total layers: linear $\to \tanh \to$ linear $\to \tanh$. In total;
                                            \begin{equation}
                                                z = \tanh\left(W_2\left(\tanh\left(W_1x + b_1\right)\right) + b_2\right)
                                            \end{equation}
                                            Again, $W_i, b_i$ are learned weight and bias parameters respectively.
                                        \end{itemize}

                                    \item \texttt{forward}
                                        \begin{itemize}
                                            \item 
                                        \end{itemize}
                                \end{itemize}

                            \item \texttt{NexusNet}
                                \begin{itemize} 
                                    \item \texttt{\_\_init\_\_}: Runs the following;
                                        \begin{itemize}
                                            \item \texttt{}
                                        \end{itemize}
                                \end{itemize}
                        \end{enumerate}

                    \item \texttt{planes.py}
                        \begin{enumerate}
                            \item
                        \end{enumerate}

                    \item \texttt{NuGraph2.py}: Contains a single class with multiple functions. This file contains the module that we have been training. This file has been organized in accordance with the prototypical LightningModule framework. Take a look at this \href{https://lightning.ai/docs/pytorch/stable/common/lightning_module.html}{\color{blue} documentation} and compare with the structure of \texttt{NuGraph2.py}
                        \begin{enumerate}
                            \item Main components; there is one initialization followed by 6 main components.
                            \begin{itemize}
                                \item \texttt{\_\_init\_\_}
                                \begin{itemize}
                                    \item Begins by calling \texttt{encoder.py}, \texttt{nexus.py}, \texttt{planes.py}.

                                    \item Chooses decoder types from \texttt{decoders} based on the boolean input parameters to the model. Appends each type to a total list.
                                \end{itemize}

                                \item \texttt{forward}

                                \item \texttt{training\_step}
                                    \begin{itemize}
                                        \item For each decoder in the list, compute respective \texttt{decoder.loss} and sets total loss to be the sum from each decoder.
                                        \item Log stuff
                                    \end{itemize}

                                \item \texttt{validation\_step}

                                \item \texttt{test\_step}

                                \item \texttt{predict\_step}

                                \item \texttt{configure\_optimizers}
                            \end{itemize}

                            \item Secondary components:
                            \begin{itemize}
                                \item \texttt{step}: I am not really sure what this does yet. It is not a method of Lightning Module as far as I know.
                            \end{itemize}
                        \end{enumerate}

                \end{enumerate}
        \end{enumerate}
\end{itemize}
